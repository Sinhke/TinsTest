{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c356ab25-0a3e-4ddd-acf1-9ca8d7b54236",
   "metadata": {},
   "source": [
    "<h1>Import</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f1c89c6-8d0f-47d4-89af-25461b500f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251a0310-0307-4b23-8403-809299f2722e",
   "metadata": {},
   "source": [
    "<h1>Load image dir</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3257c91-678d-4a39-96ad-548529b3ee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_folder_dir = \"./Images/Tin\"\n",
    "list_img_name = os.listdir(img_folder_dir)\n",
    "list_img_dir = []\n",
    "for img_name in list_img_name:\n",
    "    list_img_dir.append(os.path.join(img_folder_dir, img_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe65d78-850c-4ffb-8e0c-67d56d262c53",
   "metadata": {},
   "source": [
    "<h1>Define support functions</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff344b24-d871-49a4-852f-4b59e43cf85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_dir):\n",
    "    return cv2.imread(img_dir)\n",
    "\n",
    "def crop_edge(image, crop_x):\n",
    "    # Get image dimensions\n",
    "    if len(image.shape) == 3:\n",
    "        height, width, _ = image.shape\n",
    "    else:\n",
    "        height, width = image.shape\n",
    "\n",
    "    # Ensure X is not larger than half of width or height\n",
    "    crop_x = min(crop_x, width // 2, height // 2)\n",
    "\n",
    "    # Crop the image (remove X pixels from each edge)\n",
    "    cropped_image = image[crop_x:height - crop_x, crop_x:width - crop_x]\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "def convert_gray(image):\n",
    "    return cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "def convert_invert(image):\n",
    "    return cv2.bitwise_not(image)        \n",
    "\n",
    "def adjust_bright(image, increase=1):\n",
    "    mean_intensity_cv2 = cv2.mean(image)[0]\n",
    "    bright = cv2.add(image, mean_intensity_cv2 * increase)\n",
    "    return bright\n",
    "\n",
    "def show_hist(image, title):\n",
    "    hist = cv2.calcHist([image], [0], None, [256], [0, 256])\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.plot(hist, color='black')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Pixel Intensity\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xlim([0, 256])\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b5c18a-599f-47cb-ba08-82eaa14f5a95",
   "metadata": {},
   "source": [
    "<h1>Define filter based threahold</h1>\n",
    "- <b><code>increase</code></b> parameter can be tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10c5380f-eb50-4544-b41e-70eb8023bf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_threadhold(image, increase=1):\n",
    "    mean_intensity_cv2 = cv2.mean(image)[0]\n",
    "    mean_intensity_cv2 = min(mean_intensity_cv2 * increase, 255) #+10%\n",
    "    _, thresh = cv2.threshold(image, mean_intensity_cv2, 255, cv2.THRESH_BINARY)\n",
    "    filtered_image = np.where(image >= mean_intensity_cv2, image, 0).astype(np.uint8)\n",
    "    return filtered_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c926e6-053f-40bc-ab7a-d3179b8d0d73",
   "metadata": {},
   "source": [
    "<h1>Define filter sobel to detect edge</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2da06460-6239-4f23-91ec-6e5e37bdc22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sobel(image, kernel):\n",
    "       \n",
    "    # Apply Sobel filter in X and Y direction\n",
    "    sobel_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=kernel)  # X-direction\n",
    "    sobel_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=kernel)  # Y-direction\n",
    "\n",
    "    # Compute the gradient magnitude\n",
    "    sobel_magnitude = np.sqrt(sobel_x**2 + sobel_y**2)\n",
    "    sobel_magnitude = np.uint8(255 * sobel_magnitude / np.max(sobel_magnitude))  # Normalize\n",
    "\n",
    "    return sobel_magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daefb3d7-cdcc-41ac-a690-922cec226af9",
   "metadata": {},
   "source": [
    "<h1>Define function to find the largest area after apply Sobel filter</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c432acc3-6220-4bdd-9e40-b5ea6e5ece2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_largest_area(image, image_threadhold):\n",
    "    # Convert to binary image (thresholding)\n",
    "    _, binary = cv2.threshold(image, 100, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Select the largest contour\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    \n",
    "    # Create a mask for the largest contour\n",
    "    mask = np.zeros_like(image)\n",
    "    cv2.drawContours(mask, [largest_contour], -1, 255, thickness=cv2.FILLED)\n",
    "    \n",
    "    # # Apply mask on the original image\n",
    "    # result = cv2.bitwise_and(image_threadhold, image_threadhold, mask=mask)\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f30575-eca1-4caf-bc6c-0c7a68131a08",
   "metadata": {},
   "source": [
    "<h1>Define function to apply the mask on image</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "7310da99-7454-448b-a986-179de39e5338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_largest_area(image, largest_area, top_cut=False, color=(0,255,0)):\n",
    "    # Set mask color (Red in BGR format)\n",
    "    mask_colored = cv2.cvtColor(largest_area, cv2.COLOR_GRAY2BGR)\n",
    "    mask_colored[np.where(largest_area == 255)] = color  # Red overlay\n",
    "\n",
    "    mask_h = mask_colored.shape[0]\n",
    "    aligned_mask = np.zeros_like(image, dtype=np.uint8)\n",
    "\n",
    "    # Place the mask at the top (y=0) and keep the width the same\n",
    "    if top_cut == None:\n",
    "        aligned_mask[:, :, :] = mask_colored  # Copy the mask into the top part\n",
    "    elif top_cut:\n",
    "        aligned_mask[-mask_h:, :, :] = mask_colored\n",
    "    else:\n",
    "        aligned_mask[:mask_h, :, :] = mask_colored  # Copy the mask into the top part\n",
    "\n",
    "    # Blend the mask and image (Alpha blending)\n",
    "    alpha = 0.5  # Transparency factor\n",
    "    overlay = cv2.addWeighted(image, 1, aligned_mask, alpha, 0)\n",
    "    return overlay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb96ed2-d4d5-4a50-8bea-f1f72ca47f20",
   "metadata": {},
   "source": [
    "<h1>Define function for bounding box (Find & Apply mask)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "8f80d0d3-bd46-4e4b-895a-e3405b070c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bbox_largest_area(largest_area):\n",
    "    # Find contours in the mask\n",
    "    contours, _ = cv2.findContours(largest_area, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Draw bounding rectangles around each detected object\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)  # Get bounding box\n",
    "\n",
    "    return [x, y, w, h]\n",
    "\n",
    "def mask_bbox(masked_result, bbox_largest_area, cut_pixel=0, color=(0,255,0)):\n",
    "    x, y, w, h = bbox_largest_area\n",
    "\n",
    "    img_h, img_w, _ = masked_result.shape\n",
    "\n",
    "    y = y + cut_pixel\n",
    "    \n",
    "    cv2.rectangle(masked_result, (x, y), (x + w, y + h), color, 1)  # Draw rectangle\n",
    "    return masked_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb194f0-0e27-407e-90ec-f24866c486f8",
   "metadata": {},
   "source": [
    "<h1>Define function to approximate caculate the length and area of Mask or Bounding box</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3c85226-8c02-46b6-94b6-6cc698e38e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_length_area(largest_area, bbox_largest_area, ratio):\n",
    "    area = np.count_nonzero(largest_area)\n",
    "    x, y, w, h = bbox_largest_area\n",
    "    length = h * ratio\n",
    "    return {\n",
    "        \"area\": area, \n",
    "        \"length\": length\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da11262-0e5b-4172-8e90-17e7c586616e",
   "metadata": {},
   "source": [
    "<h1>Main process</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "4397f983-9ace-4eb0-b494-14937e62f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_area_liver = pd.DataFrame(columns=['img_dir','length','area'])\n",
    "length_area_intestine = pd.DataFrame(columns=['img_dir','length','area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "dc3baa5b-3ab9-4d02-a351-9247749d1524",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 39/39 [00:00<00:00, 253.55it/s]\n"
     ]
    }
   ],
   "source": [
    "for img_dir in tqdm(list_img_dir):\n",
    "    # img_dir = \"./Images/Tin/242.b0.1.20250108_frame0000198_obj_8.png\"\n",
    "    image = load_image(img_dir)\n",
    "    image = crop_edge(image, 1)\n",
    "    image_gray = convert_gray(image)\n",
    "    image_invert = convert_invert(image_gray)\n",
    "    \n",
    "    image_threadhold = filter_threadhold(image_invert, increase=1.2)\n",
    "    \n",
    "    image_sobel = filter_sobel(image_threadhold, kernel=1)\n",
    "\n",
    "    largest_area = find_largest_area(image_sobel, image_threadhold)\n",
    "\n",
    "    bbox_largest_area = find_bbox_largest_area(largest_area)\n",
    "    \n",
    "    masked_result = mask_largest_area(image, largest_area, top_cut=None)\n",
    "    masked_result = mask_bbox(masked_result, bbox_largest_area)\n",
    "\n",
    "    length_pixel_ratio = 2 / max(image.shape) # 2cm / 100 pixel ~ 0.02cm per pixel \n",
    "    \n",
    "    info_length_area = cal_length_area(largest_area, bbox_largest_area, ratio=length_pixel_ratio)\n",
    "\n",
    "    length_area_liver = pd.concat([length_area_liver, pd.DataFrame([{\n",
    "        \"img_dir\": img_dir,\n",
    "        \"length\": info_length_area[\"length\"],\n",
    "        \"area\": info_length_area[\"area\"]\n",
    "    }])], ignore_index=True)\n",
    "\n",
    "\n",
    "    # Get image height and width\n",
    "    img_h, img_w = image_invert.shape  # (height, width, channels)\n",
    "    top_cut = False\n",
    "    cut_pixel = 0\n",
    "    x, y, w, h = bbox_largest_area\n",
    "    # Determine cropping based on bbox position\n",
    "    if y + h / 2 < img_h / 2:\n",
    "        # BBox is in the top half → Keep only the lower half (crop below bbox)\n",
    "        image_cut = image_invert[y + h :, :]\n",
    "        cut_pixel = y + h\n",
    "        top_cut = True\n",
    "    else:\n",
    "        # BBox is in the lower half → Keep only the upper half (crop above bbox)\n",
    "        image_cut = image_invert[:y, :]\n",
    "\n",
    "    image_cut_threadhold = filter_threadhold(image_cut, increase=1.05)\n",
    "    image_cut_sobel = filter_sobel(image_cut_threadhold, kernel=1)\n",
    "    cut_largest_area = find_largest_area(image_cut_sobel, image_threadhold)\n",
    "    cut_bbox_largest_area = find_bbox_largest_area(cut_largest_area)\n",
    "    \n",
    "    cut_masked_result = mask_largest_area(masked_result, cut_largest_area, top_cut=top_cut, color=(255,0,0))\n",
    "    cut_masked_result = mask_bbox(cut_masked_result, cut_bbox_largest_area, cut_pixel=cut_pixel, color=(255,0,0))    \n",
    "\n",
    "    cut_info_length_area = cal_length_area(cut_largest_area, cut_bbox_largest_area, ratio=length_pixel_ratio)\n",
    "\n",
    "    length_area_intestine = pd.concat([length_area_intestine, pd.DataFrame([{\n",
    "        \"img_dir\": img_dir,\n",
    "        \"length\": cut_info_length_area[\"length\"],\n",
    "        \"area\": cut_info_length_area[\"area\"]\n",
    "    }])], ignore_index=True)\n",
    "\n",
    "    cv2.imwrite(os.path.join(\"./Images/Results\", img_dir.split(\"/\")[-1]), cut_masked_result)\n",
    "\n",
    "    length_area_liver.to_csv('./Images/Results/length_area_liver.csv', index=False)\n",
    "    length_area_intestine.to_csv('./Images/Results/length_area_intestine.csv', index=False)\n",
    "    # break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
